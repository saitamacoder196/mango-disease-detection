{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phân đoạn (Segmentation) bệnh trên da xoài\n",
    "\n",
    "Notebook này trình bày quy trình hoàn chỉnh cho mô hình phân đoạn bệnh trên da xoài, bao gồm các giai đoạn:\n",
    "1. Chuẩn bị môi trường và cài đặt thư viện\n",
    "2. Tải và chuẩn bị dữ liệu\n",
    "3. Khám phá và trực quan hóa dữ liệu\n",
    "4. Tạo và huấn luyện các mô hình phân đoạn\n",
    "5. Đánh giá và so sánh mô hình\n",
    "6. Sử dụng mô hình để dự đoán\n",
    "\n",
    "Mô hình phân đoạn cho phép phát hiện nhiều loại bệnh cùng lúc trên một quả xoài cũng như xác định chính xác vị trí và diện tích bị nhiễm bệnh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chuẩn bị môi trường và cài đặt thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cài đặt các thư viện cần thiết\n",
    "!pip install segmentation-models\n",
    "!pip install albumentations\n",
    "!pip install opencv-python\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install pyyaml\n",
    "!pip install tensorflow>=2.4.0\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import glob\n",
    "import segmentation_models as sm\n",
    "import albumentations as A\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "# Đặt seed cho tính khả tái\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Kiểm tra GPU\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tải và chuẩn bị dữ liệu\n",
    "### 2.1. Thiết lập cấu trúc thư mục"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tạo cấu trúc thư mục cho dữ liệu phân đoạn\n",
    "def setup_directory_structure(base_dir='data'):\n",
    "    \"\"\"Thiết lập cấu trúc thư mục cho dữ liệu phân đoạn.\"\"\"\n",
    "    # Tạo thư mục chính\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    # Tạo các thư mục con\n",
    "    for directory in ['raw', 'segmentation/images', 'segmentation/masks', 'segmentation/annotations',\n",
    "                      'segmentation/train/images', 'segmentation/train/masks',\n",
    "                      'segmentation/val/images', 'segmentation/val/masks',\n",
    "                      'segmentation/test/images', 'segmentation/test/masks',\n",
    "                      'models']:\n",
    "        os.makedirs(os.path.join(base_dir, directory), exist_ok=True)\n",
    "    \n",
    "    print(f\"Cấu trúc thư mục đã được tạo tại {base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Thiết lập cấu trúc thư mục\n",
    "setup_directory_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Xử lý dữ liệu annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Thiết lập mapping cho các nhãn bệnh\n",
    "LABEL_MAPPING = {\n",
    "    \"background\": 0,  # Nền (không bệnh)\n",
    "    \"DC\": 1,          # Da cám\n",
    "    \"DE\": 2,          # Da ếch\n",
    "    \"DD\": 3,          # Đóm đen\n",
    "    \"TT\": 4,          # Thán thư\n",
    "    \"RD\": 5,          # Rùi đụt\n",
    "}\n",
    "\n",
    "CLASS_NAMES = [\"background\", \"da_cam\", \"da_ech\", \"dom_den\", \"than_thu\", \"rui_dut\"]\n",
    "\n",
    "# Màu cho các lớp (RGB)\n",
    "COLORS = [\n",
    "    [0, 0, 0],      # Background - đen\n",
    "    [255, 0, 0],    # Da cám - đỏ\n",
    "    [0, 255, 0],    # Da ếch - xanh lá\n",
    "    [0, 0, 255],    # Đóm đen - xanh dương\n",
    "    [255, 255, 0],  # Thán thư - vàng\n",
    "    [255, 0, 255]   # Rùi đụt - tím\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def process_json_to_mask(json_path, output_size=(512, 512), label_mapping=LABEL_MAPPING):\n",
    "    \"\"\"Chuyển đổi file JSON annotation thành mask.\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Lấy kích thước ảnh gốc\n",
    "        img_height = data.get('imageHeight', output_size[0])\n",
    "        img_width = data.get('imageWidth', output_size[1])\n",
    "        \n",
    "        # Tạo mask trống\n",
    "        mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "        \n",
    "        # Vẽ các đa giác lên mask\n",
    "        for shape in data.get('shapes', []):\n",
    "            label = shape.get('label')\n",
    "            points = shape.get('points')\n",
    "            \n",
    "            # Chuyển đổi label thành ID nếu có mapping\n",
    "            label_id = label_mapping.get(label, 1) if label_mapping else 1\n",
    "            \n",
    "            # Chuyển đổi points thành định dạng phù hợp cho cv2.fillPoly\n",
    "            points_array = np.array(points, dtype=np.int32)\n",
    "            \n",
    "            # Vẽ polygon\n",
    "            cv2.fillPoly(mask, [points_array], label_id)\n",
    "        \n",
    "        # Resize mask về kích thước mong muốn\n",
    "        if output_size and (img_height != output_size[0] or img_width != output_size[1]):\n",
    "            mask = cv2.resize(mask, output_size, interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi xử lý file {json_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Chuẩn bị dữ liệu từ nguồn\n",
    "\n",
    "Trong phần này, bạn cần chỉ định nguồn dữ liệu thực tế. \n",
    "- Thay `input_dir` bằng đường dẫn thực tế đến dữ liệu của bạn\n",
    "- Dữ liệu gồm ảnh và file annotation JSON (định dạng Labelme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Chỉ định đường dẫn đến dữ liệu nguồn\n",
    "input_dir = \"path/to/raw/data\"  # Thay bằng đường dẫn thực tế\n",
    "output_dir = \"data\"\n",
    "img_size = (512, 512)\n",
    "val_split = 0.15\n",
    "test_split = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def collect_data_files(input_dir):\n",
    "    \"\"\"Thu thập tất cả các cặp file ảnh và json từ thư mục đầu vào.\"\"\"\n",
    "    print(f\"Đang quét thư mục {input_dir} để tìm file ảnh và annotation...\")\n",
    "    \n",
    "    image_files = []\n",
    "    json_files = []\n",
    "    \n",
    "    # Duyệt qua tất cả các thư mục và tìm file\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            # Thu thập file ảnh\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_files.append(file_path)\n",
    "            # Thu thập file json\n",
    "            elif file.lower().endswith('.json'):\n",
    "                json_files.append(file_path)\n",
    "    \n",
    "    print(f\"Đã tìm thấy {len(image_files)} file ảnh và {len(json_files)} file annotation.\")\n",
    "    return image_files, json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def match_image_annotation(image_files, json_files):\n",
    "    \"\"\"Ghép cặp file ảnh và file annotation.\"\"\"\n",
    "    print(\"Đang ghép cặp file ảnh và annotation...\")\n",
    "    \n",
    "    # Tạo dict lưu tên file ảnh và đường dẫn\n",
    "    image_dict = {}\n",
    "    for img_path in image_files:\n",
    "        img_name = os.path.basename(img_path)\n",
    "        image_dict[img_name] = img_path\n",
    "    \n",
    "    # Tìm file json tương ứng với mỗi ảnh\n",
    "    matched_pairs = []\n",
    "    for json_path in json_files:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                json_data = json.load(f)\n",
    "                # Lấy tên file ảnh từ imagePath trong json\n",
    "                image_path = json_data.get('imagePath', '')\n",
    "                if not image_path:\n",
    "                    continue\n",
    "                \n",
    "                # Chuẩn hóa đường dẫn và lấy tên file\n",
    "                image_name = os.path.basename(image_path.replace('\\\\', '/'))\n",
    "                \n",
    "                # Tìm file ảnh tương ứng\n",
    "                if image_name in image_dict:\n",
    "                    matched_pairs.append((image_dict[image_name], json_path))\n",
    "                else:\n",
    "                    # Trường hợp tên file trong json không khớp chính xác\n",
    "                    # Tìm file có tên gần giống\n",
    "                    potential_matches = [img for img in image_dict.keys() \n",
    "                                        if os.path.splitext(img)[0] in os.path.splitext(image_name)[0] \n",
    "                                        or os.path.splitext(image_name)[0] in os.path.splitext(img)[0]]\n",
    "                    if potential_matches:\n",
    "                        matched_pairs.append((image_dict[potential_matches[0]], json_path))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Lỗi khi đọc file {json_path}. Bỏ qua.\")\n",
    "    \n",
    "    print(f\"Đã ghép được {len(matched_pairs)} cặp ảnh và annotation.\")\n",
    "    return matched_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def process_single_pair(img_path, json_path, images_dir, masks_dir, img_size, label_mapping):\n",
    "    \"\"\"Xử lý một cặp file ảnh và annotation.\"\"\"\n",
    "    try:\n",
    "        # Lấy tên file gốc\n",
    "        img_name = os.path.basename(img_path)\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "        \n",
    "        # Đọc và resize ảnh\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Không thể đọc ảnh {img_path}\")\n",
    "            return\n",
    "        \n",
    "        img_resized = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Tạo mask từ file json\n",
    "        mask = process_json_to_mask(json_path, img_size, label_mapping)\n",
    "        if mask is None:\n",
    "            print(f\"Không thể tạo mask từ {json_path}\")\n",
    "            return\n",
    "        \n",
    "        # Lưu ảnh và mask\n",
    "        cv2.imwrite(os.path.join(images_dir, f\"{base_name}.jpg\"), img_resized)\n",
    "        cv2.imwrite(os.path.join(masks_dir, f\"{base_name}.png\"), mask)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi xử lý {img_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    # Thu thập và ghép cặp file\n",
    "    image_files, json_files = collect_data_files(input_dir)\n",
    "    matched_pairs = match_image_annotation(image_files, json_files)\n",
    "    \n",
    "    # Chia dữ liệu thành train, val, test\n",
    "    random.shuffle(matched_pairs)\n",
    "    n_total = len(matched_pairs)\n",
    "    n_test = int(n_total * test_split)\n",
    "    n_val = int(n_total * val_split)\n",
    "    n_train = n_total - n_test - n_val\n",
    "    \n",
    "    train_pairs = matched_pairs[:n_train]\n",
    "    val_pairs = matched_pairs[n_train:n_train+n_val]\n",
    "    test_pairs = matched_pairs[n_train+n_val:]\n",
    "    \n",
    "    print(f\"Chia dữ liệu: {n_train} train, {n_val} validation, {n_test} test\")\n",
    "    \n",
    "    # Xử lý từng phần\n",
    "    for subset, pairs in zip(['train', 'val', 'test'], [train_pairs, val_pairs, test_pairs]):\n",
    "        images_dir = os.path.join(output_dir, f'segmentation/{subset}/images')\n",
    "        masks_dir = os.path.join(output_dir, f'segmentation/{subset}/masks')\n",
    "        \n",
    "        for img_path, json_path in tqdm(pairs, desc=f\"Xử lý {subset}\"):\n",
    "            process_single_pair(\n",
    "                img_path, \n",
    "                json_path, \n",
    "                images_dir, \n",
    "                masks_dir, \n",
    "                img_size, \n",
    "                LABEL_MAPPING\n",
    "            )\n",
    "    \n",
    "    # Lưu tất cả vào thư mục raw để tham khảo\n",
    "    raw_dir = os.path.join(output_dir, 'raw')\n",
    "    for img_path, json_path in tqdm(matched_pairs, desc=\"Sao chép raw data\"):\n",
    "        # Sao chép file ảnh\n",
    "        img_name = os.path.basename(img_path)\n",
    "        shutil.copy2(img_path, os.path.join(raw_dir, img_name))\n",
    "        \n",
    "        # Sao chép file json\n",
    "        json_name = os.path.basename(json_path)\n",
    "        shutil.copy2(json_path, os.path.join(raw_dir, json_name))\n",
    "    \n",
    "    print(\"Hoàn thành xử lý dữ liệu!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Thực hiện chuẩn bị dữ liệu\n",
    "import shutil  # Import thư viện shutil cho việc copy file\n",
    "\n",
    "# Chỉ chạy khi có dữ liệu thực tế. Bỏ comment dòng dưới khi sử dụng\n",
    "# prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Khám phá và trực quan hóa dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_sample(img_path, mask_path):\n",
    "    \"\"\"Hiển thị một mẫu dữ liệu với ảnh gốc và mask.\"\"\"\n",
    "    # Đọc ảnh và mask\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Tạo mask màu để hiển thị\n",
    "    colored_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "    for class_idx, color in enumerate(COLORS):\n",
    "        colored_mask[mask == class_idx] = color\n",
    "    \n",
    "    # Tạo ảnh overlay\n",
    "    alpha = 0.6\n",
    "    overlay = cv2.addWeighted(img, 1-alpha, colored_mask, alpha, 0)\n",
    "    \n",
    "    # Hiển thị\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Ảnh gốc\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(colored_mask)\n",
    "    plt.title(\"Mask\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(\"Overlay\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tính tỷ lệ diện tích từng loại bệnh\n",
    "    total_pixels = mask.size\n",
    "    class_areas = {}\n",
    "    \n",
    "    for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "        pixel_count = np.sum(mask == class_idx)\n",
    "        percentage = (pixel_count / total_pixels) * 100\n",
    "        class_areas[class_name] = percentage\n",
    "    \n",
    "    # Hiển thị tỷ lệ diện tích\n",
    "    print(\"Tỷ lệ diện tích từng loại bệnh:\")\n",
    "    for class_name, percentage in class_areas.items():\n",
    "        if percentage > 0:\n",
    "            print(f\"{class_name}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def explore_dataset(subset='train'):\n",
    "    \"\"\"Khám phá dataset và hiển thị một số mẫu.\"\"\"\n",
    "    images_dir = os.path.join(output_dir, f'segmentation/{subset}/images')\n",
    "    masks_dir = os.path.join(output_dir, f'segmentation/{subset}/masks')\n",
    "    \n",
    "    # Lấy danh sách file ảnh\n",
    "    image_files = sorted(glob.glob(os.path.join(images_dir, \"*.jpg\")))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"Không tìm thấy file ảnh trong {images_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Tìm thấy {len(image_files)} ảnh trong tập {subset}\")\n",
    "    \n",
    "    # Hiển thị một số mẫu ngẫu nhiên\n",
    "    num_samples = min(5, len(image_files))\n",
    "    sample_indices = np.random.choice(len(image_files), num_samples, replace=False)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        img_path = image_files[idx]\n",
    "        img_name = os.path.basename(img_path)\n",
    "        mask_path = os.path.join(masks_dir, os.path.splitext(img_name)[0] + \".png\")\n",
    "        \n",
    "        print(f\"\\nMẫu: {img_name}\")\n",
    "        visualize_sample(img_path, mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Khám phá tập huấn luyện\n",
    "explore_dataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Khám phá tập validation\n",
    "explore_dataset('val')\n",
    "\n",
    "# Phân tích phân phối lớp trong dataset\n",
    "def analyze_class_distribution():\n",
    "    \"\"\"Phân tích phân phối các lớp trong dataset.\"\"\"\n",
    "    subsets = ['train', 'val', 'test']\n",
    "    class_distribution = {subset: {class_name: 0 for class_name in CLASS_NAMES} for subset in subsets}\n",
    "    total_pixels = {subset: 0 for subset in subsets}\n",
    "    \n",
    "    for subset in subsets:\n",
    "        masks_dir = os.path.join(output_dir, f'segmentation/{subset}/masks')\n",
    "        mask_files = glob.glob(os.path.join(masks_dir, \"*.png\"))\n",
    "        \n",
    "        for mask_path in tqdm(mask_files, desc=f\"Phân tích {subset}\"):\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if mask is None:\n",
    "                continue\n",
    "                \n",
    "            total_pixels[subset] += mask.size\n",
    "            \n",
    "            for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "                pixel_count = np.sum(mask == class_idx)\n",
    "                class_distribution[subset][class_name] += pixel_count\n",
    "    \n",
    "    # Tính tỷ lệ phần trăm và hiển thị kết quả\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    for i, subset in enumerate(subsets):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        \n",
    "        percentages = [\n",
    "            (class_distribution[subset][class_name] / total_pixels[subset] * 100)\n",
    "            for class_name in CLASS_NAMES if class_distribution[subset][class_name] > 0\n",
    "        ]\n",
    "        \n",
    "        labels = [\n",
    "            class_name \n",
    "            for class_name in CLASS_NAMES \n",
    "            if class_distribution[subset][class_name] > 0\n",
    "        ]\n",
    "        \n",
    "        # Sử dụng màu tương ứng cho từng lớp bệnh\n",
    "        colors = [tuple(c/255 for c in COLORS[i]) for i, class_name in enumerate(labels)]\n",
    "        \n",
    "        plt.pie(percentages, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "        plt.title(f'Phân phối lớp trong tập {subset}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Hiển thị bảng số lượng pixel\n",
    "    print(\"Số lượng pixel cho từng lớp bệnh:\")\n",
    "    print(f\"{'Lớp':<15} {'Train':<15} {'Validation':<15} {'Test':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for class_name in CLASS_NAMES:\n",
    "        train_count = class_distribution['train'][class_name]\n",
    "        val_count = class_distribution['val'][class_name]\n",
    "        test_count = class_distribution['test'][class_name]\n",
    "        \n",
    "        print(f\"{class_name:<15} {train_count:<15} {val_count:<15} {test_count:<15}\")\n",
    "\n",
    "analyze_class_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Hiển thị tất cả các loại bệnh và màu sắc tương ứng\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i, (class_name, color) in enumerate(zip(CLASS_NAMES, COLORS)):\n",
    "    plt.subplot(1, len(CLASS_NAMES), i+1)\n",
    "    plt.imshow([[color]])\n",
    "    plt.title(class_name)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Hiển thị các ảnh và mask theo từng loại bệnh\n",
    "def show_samples_by_disease():\n",
    "    \"\"\"Hiển thị các ảnh mẫu theo từng loại bệnh.\"\"\"\n",
    "    masks_dir = os.path.join(output_dir, 'segmentation/train/masks')\n",
    "    images_dir = os.path.join(output_dir, 'segmentation/train/images')\n",
    "    \n",
    "    mask_files = glob.glob(os.path.join(masks_dir, \"*.png\"))\n",
    "    \n",
    "    # Lưu các ảnh có từng loại bệnh\n",
    "    samples_by_disease = {class_name: [] for class_name in CLASS_NAMES[1:]}  # Bỏ qua background\n",
    "    \n",
    "    for mask_path in mask_files:\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            continue\n",
    "        \n",
    "        base_name = os.path.splitext(os.path.basename(mask_path))[0]\n",
    "        img_path = os.path.join(images_dir, f\"{base_name}.jpg\")\n",
    "        \n",
    "        for class_idx, class_name in enumerate(CLASS_NAMES[1:], 1):  # Bắt đầu từ lớp thứ 1 (bỏ qua background)\n",
    "            if np.sum(mask == class_idx) > 1000:  # Có đủ pixel của loại bệnh này\n",
    "                samples_by_disease[class_name].append((img_path, mask_path))\n",
    "                break  # Chỉ lấy loại bệnh chính\n",
    "    \n",
    "    # Hiển thị mẫu cho từng loại bệnh\n",
    "    for class_name, samples in samples_by_disease.items():\n",
    "        if not samples:\n",
    "            print(f\"Không có mẫu cho bệnh {class_name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nMẫu cho bệnh {class_name} (tổng số: {len(samples)}):\")\n",
    "        \n",
    "        # Chọn ngẫu nhiên 1 mẫu\n",
    "        if len(samples) > 0:\n",
    "            sample = random.choice(samples)\n",
    "            visualize_sample(sample[0], sample[1])\n",
    "\n",
    "# Hiển thị mẫu theo từng loại bệnh\n",
    "show_samples_by_disease()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tạo mask màu từ mask grayscale\n",
    "def create_colored_mask(mask):\n",
    "    \"\"\"Tạo mask màu từ mask grayscale.\"\"\"\n",
    "    colored_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "    for class_idx, color in enumerate(COLORS):\n",
    "        colored_mask[mask == class_idx] = color\n",
    "    return colored_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tạo và huấn luyện các mô hình phân đoạn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tạo cấu hình mô hình segmentation\n",
    "segmentation_config = {\n",
    "    \"data\": {\n",
    "        \"train_dir\": os.path.join(output_dir, \"segmentation/train\"),\n",
    "        \"validation_dir\": os.path.join(output_dir, \"segmentation/val\"),\n",
    "        \"test_dir\": os.path.join(output_dir, \"segmentation/test\"),\n",
    "        \"img_size\": list(img_size),\n",
    "        \"use_augmentation\": True\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"input_shape\": [*img_size, 3],\n",
    "        \"num_classes\": len(CLASS_NAMES),\n",
    "        \"save_dir\": os.path.join(output_dir, \"models\"),\n",
    "        \"class_names\": CLASS_NAMES,\n",
    "        \"segmentation_model\": {\n",
    "            \"architecture\": \"unet\",  # Hoặc \"fpn\", \"pspnet\", \"deeplabv3\"\n",
    "            \"encoder\": \"resnet34\",   # Hoặc \"resnet50\", \"efficientnetb0\", \"mobilenetv2\"\n",
    "            \"encoder_weights\": \"imagenet\",\n",
    "            \"activation\": \"softmax\"\n",
    "        }\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 50,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"early_stopping_patience\": 10,\n",
    "        \"reduce_lr_patience\": 5,\n",
    "        \"use_augmentation\": True,\n",
    "        \"class_weights\": None,\n",
    "        \"loss\": \"categorical_crossentropy\",\n",
    "        \"optimizer\": \"adam\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Lưu cấu hình\n",
    "os.makedirs(os.path.join(output_dir, \"configs\"), exist_ok=True)\n",
    "with open(os.path.join(output_dir, \"configs\", \"segmentation_config.yaml\"), \"w\") as f:\n",
    "    yaml.dump(segmentation_config, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Tạo bộ nạp dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Bộ nạp dữ liệu cho segmentation\n",
    "class SegmentationDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Bộ nạp dữ liệu cho model segmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, images_dir, masks_dir, batch_size=8, img_size=(512, 512), \n",
    "                num_classes=6, augmentation=False, augmentation_config=None, shuffle=True):\n",
    "        \"\"\"Khởi tạo generator.\"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "        # Lấy danh sách file ảnh\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(images_dir, \"*.jpg\")))\n",
    "        \n",
    "        # Lấy danh sách file mask tương ứng\n",
    "        self.mask_paths = []\n",
    "        for img_path in self.image_paths:\n",
    "            img_name = os.path.basename(img_path)\n",
    "            base_name = os.path.splitext(img_name)[0]\n",
    "            mask_path = os.path.join(masks_dir, f\"{base_name}.png\")\n",
    "            if os.path.exists(mask_path):\n",
    "                self.mask_paths.append(mask_path)\n",
    "            else:\n",
    "                # Nếu không tìm thấy mask tương ứng, loại bỏ ảnh khỏi danh sách\n",
    "                self.image_paths.remove(img_path)\n",
    "        \n",
    "        # Tạo albumentation cho augmentation\n",
    "        if augmentation:\n",
    "            self.aug_transform = A.Compose([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "                A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "                A.RandomScale(scale_limit=0.2, p=0.5),\n",
    "                A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
    "                A.RandomCrop(height=img_size[0], width=img_size[1], p=0.7)\n",
    "            ])\n",
    "        \n",
    "        # Tạo indices\n",
    "        self.indices = np.arange(len(self.image_paths))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Trả về số batch trong một epoch.\"\"\"\n",
    "        return len(self.image_paths) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Trả về một batch dữ liệu.\"\"\"\n",
    "        # Lấy indices của batch hiện tại\n",
    "        indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        # Khởi tạo batch data\n",
    "        batch_imgs = np.zeros((self.batch_size, *self.img_size, 3), dtype=np.float32)\n",
    "        batch_masks = np.zeros((self.batch_size, *self.img_size, self.num_classes), dtype=np.float32)\n",
    "        \n",
    "        # Nạp dữ liệu\n",
    "        for i, idx in enumerate(indices):\n",
    "            # Đọc ảnh và mask\n",
    "            img = cv2.imread(self.image_paths[idx])\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, self.img_size)\n",
    "            \n",
    "            mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "            mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            # Áp dụng augmentation nếu được yêu cầu\n",
    "            if self.augmentation:\n",
    "                augmented = self.aug_transform(image=img, mask=mask)\n",
    "                img = augmented['image']\n",
    "                mask = augmented['mask']\n",
    "            \n",
    "            # Chuẩn hóa ảnh\n",
    "            img = img / 255.0\n",
    "            \n",
    "            # One-hot encoding cho mask\n",
    "            mask_onehot = to_categorical(mask, num_classes=self.num_classes)\n",
    "            \n",
    "            # Thêm vào batch\n",
    "            batch_imgs[i] = img\n",
    "            batch_masks[i] = mask_onehot\n",
    "        \n",
    "        return batch_imgs, batch_masks\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Được gọi khi kết thúc một epoch.\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_data_generators():\n",
    "    \"\"\"Tạo generators cho training, validation và test.\"\"\"\n",
    "    # Lấy config\n",
    "    data_config = segmentation_config['data']\n",
    "    training_config = segmentation_config['training']\n",
    "    \n",
    "    # Đường dẫn đến dữ liệu\n",
    "    train_images_dir = os.path.join(data_config['train_dir'], 'images')\n",
    "    train_masks_dir = os.path.join(data_config['train_dir'], 'masks')\n",
    "    val_images_dir = os.path.join(data_config['validation_dir'], 'images')\n",
    "    val_masks_dir = os.path.join(data_config['validation_dir'], 'masks')\n",
    "    test_images_dir = os.path.join(data_config['test_dir'], 'images')\n",
    "    test_masks_dir = os.path.join(data_config['test_dir'], 'masks')\n",
    "    \n",
    "    # Số lớp và kích thước ảnh\n",
    "    num_classes = segmentation_config['model']['num_classes']\n",
    "    img_size = tuple(data_config['img_size'])\n",
    "    batch_size = training_config['batch_size']\n",
    "    use_augmentation = training_config['use_augmentation']\n",
    "    \n",
    "    # Tạo generator\n",
    "    train_gen = SegmentationDataGenerator(\n",
    "        train_images_dir, \n",
    "        train_masks_dir, \n",
    "        batch_size=batch_size, \n",
    "        img_size=img_size, \n",
    "        num_classes=num_classes, \n",
    "        augmentation=use_augmentation\n",
    "    )\n",
    "    \n",
    "    val_gen = SegmentationDataGenerator(\n",
    "        val_images_dir, \n",
    "        val_masks_dir, \n",
    "        batch_size=batch_size, \n",
    "        img_size=img_size, \n",
    "        num_classes=num_classes, \n",
    "        augmentation=False\n",
    "    )\n",
    "    \n",
    "    test_gen = SegmentationDataGenerator(\n",
    "        test_images_dir, \n",
    "        test_masks_dir, \n",
    "        batch_size=batch_size, \n",
    "        img_size=img_size, \n",
    "        num_classes=num_classes, \n",
    "        augmentation=False,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_gen, val_gen, test_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Định nghĩa mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_segmentation_model():\n",
    "    \"\"\"Xây dựng mô hình phân đoạn.\"\"\"\n",
    "    # Lấy config\n",
    "    model_config = segmentation_config['model']\n",
    "    segmentation_model_config = model_config['segmentation_model']\n",
    "    \n",
    "    # Thiết lập framework cho segmentation-models\n",
    "    sm.set_framework('tf.keras')\n",
    "    \n",
    "    # Tùy chọn mô hình\n",
    "    input_shape = tuple(model_config['input_shape'])\n",
    "    num_classes = model_config['num_classes']\n",
    "    architecture = segmentation_model_config['architecture']\n",
    "    encoder = segmentation_model_config['encoder']\n",
    "    encoder_weights = segmentation_model_config.get('encoder_weights', 'imagenet')\n",
    "    activation = segmentation_model_config.get('activation', 'softmax')\n",
    "    \n",
    "    # Chọn kiến trúc mô hình\n",
    "    if architecture.lower() == 'unet':\n",
    "        model_fn = sm.Unet\n",
    "    elif architecture.lower() == 'fpn':\n",
    "        model_fn = sm.FPN\n",
    "    elif architecture.lower() == 'pspnet':\n",
    "        model_fn = sm.PSPNet\n",
    "    elif architecture.lower() == 'deeplabv3':\n",
    "        model_fn = sm.DeepLabV3\n",
    "    elif architecture.lower() == 'linknet':\n",
    "        model_fn = sm.Linknet\n",
    "    else:\n",
    "        raise ValueError(f\"Kiến trúc {architecture} không được hỗ trợ\")\n",
    "    \n",
    "    # Xây dựng mô hình\n",
    "    model = model_fn(\n",
    "        encoder_name=encoder,\n",
    "        encoder_weights=encoder_weights,\n",
    "        classes=num_classes,\n",
    "        activation=activation,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Tạo các metrics\n",
    "def get_segmentation_metrics():\n",
    "    \"\"\"Trả về các metrics phù hợp cho mô hình phân đoạn.\"\"\"\n",
    "    return [\n",
    "        sm.metrics.IOUScore(threshold=0.5),  # IoU score\n",
    "        sm.metrics.FScore(threshold=0.5),    # F1 score\n",
    "        'accuracy'\n",
    "    ]\n",
    "\n",
    "# Tạo hàm loss\n",
    "def get_segmentation_loss(loss_name='categorical_crossentropy', class_weights=None):\n",
    "    \"\"\"Trả về hàm loss phù hợp cho mô hình phân đoạn.\"\"\"\n",
    "    if loss_name == 'categorical_crossentropy':\n",
    "        return 'categorical_crossentropy'\n",
    "    elif loss_name == 'dice_loss':\n",
    "        return sm.losses.DiceLoss(class_weights=class_weights)\n",
    "    elif loss_name == 'focal_loss':\n",
    "        return sm.losses.CategoricalFocalLoss()\n",
    "    elif loss_name == 'jaccard_loss':\n",
    "        return sm.losses.JaccardLoss(class_weights=class_weights)\n",
    "    elif loss_name == 'combined_loss':\n",
    "        dice_loss = sm.losses.DiceLoss(class_weights=class_weights)\n",
    "        focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "        return dice_loss + focal_loss\n",
    "    else:\n",
    "        raise ValueError(f\"Loss {loss_name} không được hỗ trợ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_segmentation_model():\n",
    "    \"\"\"Huấn luyện mô hình phân đoạn.\"\"\"\n",
    "    # Tạo generators\n",
    "    train_gen, val_gen, _ = create_data_generators()\n",
    "    \n",
    "    # Tạo mô hình\n",
    "    model = build_segmentation_model()\n",
    "    \n",
    "    # Lấy config\n",
    "    training_config = segmentation_config['training']\n",
    "    model_config = segmentation_config['model']\n",
    "    \n",
    "    # Tạo optimizer\n",
    "    if training_config['optimizer'].lower() == 'adam':\n",
    "        optimizer = Adam(learning_rate=training_config['learning_rate'])\n",
    "    elif training_config['optimizer'].lower() == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=training_config['learning_rate'])\n",
    "    elif training_config['optimizer'].lower() == 'sgd':\n",
    "        optimizer = SGD(learning_rate=training_config['learning_rate'], momentum=0.9)\n",
    "    else:\n",
    "        optimizer = Adam(learning_rate=training_config['learning_rate'])\n",
    "    \n",
    "    # Biên dịch mô hình\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=get_segmentation_loss(training_config['loss'], training_config['class_weights']),\n",
    "        metrics=get_segmentation_metrics()\n",
    "    )\n",
    "    \n",
    "    # In thông tin mô hình\n",
    "    model.summary()\n",
    "    \n",
    "    # Tạo các callbacks\n",
    "    callbacks = []\n",
    "    \n",
    "    # Model checkpoint\n",
    "    os.makedirs(model_config['save_dir'], exist_ok=True)\n",
    "    model_path = os.path.join(model_config['save_dir'], 'segmentation_model.h5')\n",
    "    callbacks.append(ModelCheckpoint(\n",
    "        model_path,\n",
    "        monitor='val_iou_score',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ))\n",
    "    \n",
    "    # Early stopping\n",
    "    callbacks.append(EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=training_config['early_stopping_patience'],\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ))\n",
    "    \n",
    "    # Reduce LR on plateau\n",
    "    callbacks.append(ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=training_config['reduce_lr_patience'],\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ))\n",
    "    \n",
    "    # TensorBoard\n",
    "    if training_config.get('use_tensorboard', True):\n",
    "        log_dir = os.path.join(output_dir, 'logs', 'segmentation', datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        callbacks.append(TensorBoard(\n",
    "            log_dir=log_dir,\n",
    "            histogram_freq=1,\n",
    "            update_freq='epoch'\n",
    "        ))\n",
    "    \n",
    "    # Huấn luyện mô hình\n",
    "    print(f\"Bắt đầu huấn luyện mô hình {segmentation_config['model']['segmentation_model']['architecture']} với encoder {segmentation_config['model']['segmentation_model']['encoder']}...\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=training_config['epochs'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Lưu lịch sử huấn luyện\n",
    "    save_training_plots(history, model_config['save_dir'])\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def save_training_plots(history, save_dir):\n",
    "    \"\"\"Lưu biểu đồ quá trình huấn luyện.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Tạo biểu đồ loss\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # IoU Score\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['iou_score'], label='Train IoU')\n",
    "    plt.plot(history.history['val_iou_score'], label='Validation IoU')\n",
    "    plt.title('IoU Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('IoU Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    # F1 Score\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['f1-score'], label='Train F1')\n",
    "    plt.plot(history.history['val_f1-score'], label='Validation F1')\n",
    "    plt.title('F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'segmentation_training_history.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Lưu lịch sử huấn luyện vào file\n",
    "    history_file = os.path.join(save_dir, 'segmentation_history.json')\n",
    "    import json\n",
    "    with open(history_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'loss': history.history['loss'],\n",
    "            'val_loss': history.history['val_loss'],\n",
    "            'iou_score': history.history['iou_score'],\n",
    "            'val_iou_score': history.history['val_iou_score'],\n",
    "            'f1-score': history.history['f1-score'],\n",
    "            'val_f1-score': history.history['val_f1-score'],\n",
    "            'accuracy': history.history['accuracy'],\n",
    "            'val_accuracy': history.history['val_accuracy']\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"Đã lưu lịch sử huấn luyện vào {history_file}\")\n",
    "\n",
    "# Chỉ huấn luyện nếu có dữ liệu thực tế\n",
    "# Bỏ comment dòng dưới khi muốn huấn luyện\n",
    "# model, history = train_segmentation_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Thử nghiệm nhiều kiến trúc mô hình khác nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def experiment_with_models():\n",
    "    \"\"\"Thử nghiệm với nhiều kiến trúc và encoder khác nhau.\"\"\"\n",
    "    # Tạo generator cho dữ liệu\n",
    "    train_gen, val_gen, _ = create_data_generators()\n",
    "    \n",
    "    # Danh sách các kiến trúc muốn thử nghiệm\n",
    "    architectures = ['unet', 'fpn', 'pspnet']\n",
    "    \n",
    "    # Danh sách các encoder muốn thử nghiệm\n",
    "    encoders = ['resnet34', 'efficientnetb0', 'mobilenetv2']\n",
    "    \n",
    "    # Cấu hình huấn luyện nhanh cho thử nghiệm\n",
    "    segmentation_config['training']['epochs'] = 15\n",
    "    segmentation_config['training']['early_stopping_patience'] = 5\n",
    "    \n",
    "    # Kết quả thử nghiệm\n",
    "    results = []\n",
    "    \n",
    "    for architecture in architectures:\n",
    "        for encoder in encoders:\n",
    "            print(f\"\\nThử nghiệm: {architecture} với encoder {encoder}\")\n",
    "            \n",
    "            # Cập nhật cấu hình\n",
    "            segmentation_config['model']['segmentation_model']['architecture'] = architecture\n",
    "            segmentation_config['model']['segmentation_model']['encoder'] = encoder\n",
    "            \n",
    "            # Tạo mô hình\n",
    "            model = build_segmentation_model()\n",
    "            \n",
    "            # Lấy config\n",
    "            training_config = segmentation_config['training']\n",
    "            \n",
    "            # Tạo optimizer\n",
    "            optimizer = Adam(learning_rate=training_config['learning_rate'])\n",
    "            \n",
    "            # Biên dịch mô hình\n",
    "            model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=get_segmentation_loss(training_config['loss'], training_config['class_weights']),\n",
    "                metrics=get_segmentation_metrics()\n",
    "            )\n",
    "            \n",
    "            # Early stopping để ngừng sớm nếu không cải thiện\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=training_config['early_stopping_patience'],\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Reduce LR\n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=training_config['reduce_lr_patience'],\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Huấn luyện\n",
    "            start_time = datetime.now()\n",
    "            history = model.fit(\n",
    "                train_gen,\n",
    "                validation_data=val_gen,\n",
    "                epochs=training_config['epochs'],\n",
    "                callbacks=[early_stopping, reduce_lr],\n",
    "                verbose=1\n",
    "            )\n",
    "            end_time = datetime.now()\n",
    "            \n",
    "            # Lưu kết quả\n",
    "            best_val_loss = min(history.history['val_loss'])\n",
    "            best_val_iou = max(history.history['val_iou_score'])\n",
    "            training_time = (end_time - start_time).total_seconds() / 60.0  # Thời gian theo phút\n",
    "            \n",
    "            results.append({\n",
    "                'architecture': architecture,\n",
    "                'encoder': encoder,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_iou': best_val_iou,\n",
    "                'epochs_trained': len(history.history['loss']),\n",
    "                'training_time_minutes': training_time\n",
    "            })\n",
    "            \n",
    "            # Lưu mô hình\n",
    "            model_save_path = os.path.join(\n",
    "                segmentation_config['model']['save_dir'], \n",
    "                f\"{architecture}_{encoder}_model.h5\"\n",
    "            )\n",
    "            model.save(model_save_path)\n",
    "            print(f\"Đã lưu mô hình vào {model_save_path}\")\n",
    "            \n",
    "            # Giải phóng bộ nhớ\n",
    "            del model\n",
    "            tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Hiển thị kết quả\n",
    "    print(\"\\nKết quả thử nghiệm:\\n\")\n",
    "    print(f\"{'Architecture':<10} {'Encoder':<15} {'Best Val Loss':<15} {'Best Val IoU':<15} {'Epochs':<10} {'Time (min)':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"{result['architecture']:<10} {result['encoder']:<15} {result['best_val_loss']:<15.4f} {result['best_val_iou']:<15.4f} {result['epochs_trained']:<10} {result['training_time_minutes']:<10.2f}\")\n",
    "    \n",
    "    # Vẽ biểu đồ so sánh\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Bar chart cho IoU Score\n",
    "    plt.subplot(2, 1, 1)\n",
    "    x = np.arange(len(results))\n",
    "    labels = [f\"{r['architecture']}_{r['encoder']}\" for r in results]\n",
    "    plt.bar(x, [r['best_val_iou'] for r in results])\n",
    "    plt.xticks(x, labels, rotation=45)\n",
    "    plt.ylabel('Best Validation IoU')\n",
    "    plt.title('So sánh IoU Score giữa các mô hình')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Bar chart cho thời gian huấn luyện\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.bar(x, [r['training_time_minutes'] for r in results])\n",
    "    plt.xticks(x, labels, rotation=45)\n",
    "    plt.ylabel('Thời gian huấn luyện (phút)')\n",
    "    plt.title('So sánh thời gian huấn luyện')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(segmentation_config['model']['save_dir'], 'model_comparison.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Thử nghiệm với nhiều mô hình (bỏ comment để chạy)\n",
    "# experiment_results = experiment_with_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Đánh giá mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_segmentation_model(model_path=None):\n",
    "    \"\"\"Đánh giá mô hình phân đoạn trên tập test.\"\"\"\n",
    "    # Tạo generator cho dữ liệu test\n",
    "    _, _, test_gen = create_data_generators()\n",
    "    \n",
    "    # Tải mô hình nếu đường dẫn được cung cấp\n",
    "    if model_path:\n",
    "        print(f\"Đang tải mô hình từ {model_path}...\")\n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects={\n",
    "                'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "                'f1-score': sm.metrics.FScore(threshold=0.5)\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        # Nếu không có đường dẫn, tạo mô hình mới (chỉ cho ví dụ)\n",
    "        print(\"Không có đường dẫn mô hình được cung cấp. Đang tạo mô hình mới...\")\n",
    "        model = build_segmentation_model()\n",
    "        \n",
    "        # Biên dịch mô hình\n",
    "        training_config = segmentation_config['training']\n",
    "        optimizer = Adam(learning_rate=training_config['learning_rate'])\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=get_segmentation_loss(training_config['loss']),\n",
    "            metrics=get_segmentation_metrics()\n",
    "        )\n",
    "    \n",
    "    # Đánh giá mô hình\n",
    "    print(\"Đang đánh giá mô hình...\")\n",
    "    results = model.evaluate(test_gen, verbose=1)\n",
    "    \n",
    "    # In kết quả\n",
    "    metrics_names = model.metrics_names\n",
    "    for name, value in zip(metrics_names, results):\n",
    "        print(f\"{name}: {value:.4f}\")\n",
    "    \n",
    "    # Lưu kết quả\n",
    "    save_dir = os.path.join(output_dir, \"evaluation_results\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"evaluation_results.txt\"), \"w\") as f:\n",
    "        for name, value in zip(metrics_names, results):\n",
    "            f.write(f\"{name}: {value:.4f}\\n\")\n",
    "    \n",
    "    # Hiển thị một số ví dụ\n",
    "    visualize_predictions(model, test_gen, save_dir)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(model, test_gen, save_dir):\n",
    "    \"\"\"Hiển thị kết quả dự đoán trên một số mẫu.\"\"\"\n",
    "    # Lấy một batch dữ liệu từ generator\n",
    "    batch_imgs, batch_masks = test_gen[0]\n",
    "    \n",
    "    # Dự đoán trên batch\n",
    "    batch_preds = model.predict(batch_imgs)\n",
    "    \n",
    "    # Chuyển đổi từ one-hot encoding về class index\n",
    "    batch_masks_argmax = np.argmax(batch_masks, axis=-1)\n",
    "    batch_preds_argmax = np.argmax(batch_preds, axis=-1)\n",
    "    \n",
    "    # Số mẫu để hiển thị\n",
    "    num_samples = min(5, len(batch_imgs))\n",
    "    \n",
    "    plt.figure(figsize=(15, 4 * num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Ảnh gốc\n",
    "        plt.subplot(num_samples, 3, 3*i+1)\n",
    "        plt.imshow(batch_imgs[i])\n",
    "        plt.title(\"Ảnh gốc\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Mask thực tế\n",
    "        plt.subplot(num_samples, 3, 3*i+2)\n",
    "        colored_mask = create_colored_mask(batch_masks_argmax[i])\n",
    "        plt.imshow(colored_mask)\n",
    "        plt.title(\"Mask thực tế\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Mask dự đoán\n",
    "        plt.subplot(num_samples, 3, 3*i+3)\n",
    "        colored_pred = create_colored_mask(batch_preds_argmax[i])\n",
    "        plt.imshow(colored_pred)\n",
    "        plt.title(\"Mask dự đoán\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"prediction_examples.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # Tính toán ma trận nhầm lẫn cho từng lớp\n",
    "    num_classes = segmentation_config['model']['num_classes']\n",
    "    conf_matrices = []\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        # Tạo mask nhị phân cho lớp class_idx\n",
    "        true_masks = (batch_masks_argmax.flatten() == class_idx).astype(int)\n",
    "        pred_masks = (batch_preds_argmax.flatten() == class_idx).astype(int)\n",
    "        \n",
    "        # Tính ma trận nhầm lẫn\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(true_masks, pred_masks, labels=[0, 1])\n",
    "        conf_matrices.append(cm)\n",
    "    \n",
    "    # Hiển thị ma trận nhầm lẫn cho từng lớp\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, (cm, class_name) in enumerate(zip(conf_matrices, CLASS_NAMES)):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "        plt.title(f'Confusion Matrix - {class_name}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"confusion_matrices.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # Tính các metric cho từng lớp\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    \n",
    "    class_metrics = []\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        # Tạo mask nhị phân cho lớp class_idx\n",
    "        true_masks = (batch_masks_argmax.flatten() == class_idx).astype(int)\n",
    "        pred_masks = (batch_preds_argmax.flatten() == class_idx).astype(int)\n",
    "        \n",
    "        # Tính các metric\n",
    "        precision = precision_score(true_masks, pred_masks, zero_division=0)\n",
    "        recall = recall_score(true_masks, pred_masks, zero_division=0)\n",
    "        f1 = f1_score(true_masks, pred_masks, zero_division=0)\n",
    "        \n",
    "        class_metrics.append({\n",
    "            'class': class_name,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        })\n",
    "    \n",
    "    # Hiển thị bảng metric cho từng lớp\n",
    "    print(\"\\nMetrics cho từng lớp:\\n\")\n",
    "    print(f\"{'Class':<15} {'Precision':<15} {'Recall':<15} {'F1 Score':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for metrics in class_metrics:\n",
    "        print(f\"{metrics['class']:<15} {metrics['precision']:<15.4f} {metrics['recall']:<15.4f} {metrics['f1_score']:<15.4f}\")\n",
    "    \n",
    "    # Lưu vào file\n",
    "    with open(os.path.join(save_dir, \"class_metrics.txt\"), \"w\") as f:\n",
    "        f.write(f\"{'Class':<15} {'Precision':<15} {'Recall':<15} {'F1 Score':<15}\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        \n",
    "        for metrics in class_metrics:\n",
    "            f.write(f\"{metrics['class']:<15} {metrics['precision']:<15.4f} {metrics['recall']:<15.4f} {metrics['f1_score']:<15.4f}\\n\")\n",
    "\n",
    "# Đánh giá mô hình (bỏ comment để chạy)\n",
    "# evaluate_results = evaluate_segmentation_model('path/to/your/segmentation_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sử dụng mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def predict_segmentation(image_path, model_path, output_path=None, overlay=True):\n",
    "    \"\"\"\n",
    "    Dự đoán phân đoạn cho một ảnh.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Đường dẫn đến ảnh cần dự đoán\n",
    "        model_path: Đường dẫn đến mô hình đã huấn luyện\n",
    "        output_path: Đường dẫn lưu kết quả (tùy chọn)\n",
    "        overlay: Có tạo overlay hay không\n",
    "        \n",
    "    Returns:\n",
    "        pred_mask: Mask dự đoán\n",
    "        overlay_img: Ảnh overlay nếu overlay=True\n",
    "    \"\"\"\n",
    "    # Lấy kích thước ảnh từ config\n",
    "    img_size = tuple(segmentation_config['model']['input_shape'][:2])\n",
    "    \n",
    "    # Đọc ảnh\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Không thể đọc ảnh từ {image_path}\")\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize ảnh\n",
    "    img_resized = cv2.resize(img, img_size)\n",
    "    \n",
    "    # Chuẩn bị đầu vào\n",
    "    img_input = img_resized / 255.0\n",
    "    img_input = np.expand_dims(img_input, axis=0)\n",
    "    \n",
    "    # Tải mô hình\n",
    "    model = tf.keras.models.load_model(\n",
    "        model_path,\n",
    "        custom_objects={\n",
    "            'iou_score': sm.metrics.IOUScore(threshold=0.5),\n",
    "            'f1-score': sm.metrics.FScore(threshold=0.5)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Dự đoán\n",
    "    pred = model.predict(img_input)[0]\n",
    "    pred_mask = np.argmax(pred, axis=-1)\n",
    "    \n",
    "    # Tạo mask màu\n",
    "    colored_mask = create_colored_mask(pred_mask)\n",
    "    \n",
    "    # Tạo overlay\n",
    "    overlay_img = None\n",
    "    if overlay:\n",
    "        alpha = 0.6\n",
    "        overlay_img = cv2.addWeighted(img_resized, 1-alpha, colored_mask, alpha, 0)\n",
    "    \n",
    "    # Lưu kết quả nếu cần\n",
    "    if output_path:\n",
    "        output_img = overlay_img if overlay else colored_mask\n",
    "        plt.imsave(output_path, output_img)\n",
    "        print(f\"Đã lưu kết quả dự đoán vào {output_path}\")\n",
    "    \n",
    "    # Hiển thị kết quả\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img_resized)\n",
    "    plt.title(\"Ảnh gốc\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(colored_mask)\n",
    "    plt.title(\"Mask dự đoán\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if overlay:\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(overlay_img)\n",
    "        plt.title(\"Overlay\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tính phần trăm diện tích cho từng lớp bệnh\n",
    "    total_pixels = pred_mask.size\n",
    "    class_areas = {}\n",
    "    \n",
    "    for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "        pixel_count = np.sum(pred_mask == class_idx)\n",
    "        percentage = (pixel_count / total_pixels) * 100\n",
    "        class_areas[class_name] = percentage\n",
    "    \n",
    "    # Hiển thị phần trăm diện tích\n",
    "    print(\"Phần trăm diện tích của từng loại bệnh:\")\n",
    "    for class_name, percentage in class_areas.items():\n",
    "        if percentage > 0:\n",
    "            print(f\"{class_name}: {percentage:.2f}%\")\n",
    "    \n",
    "    return pred_mask, overlay_img if overlay else colored_mask"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
